# Comprehensive Evaluation of Multimodal Embedding Models on MS-COCO

**Authors:** Ekrem Krem
**Date:** November 2025
**Dataset:** MS-COCO Karpathy (5,000 images) + Winoground (400 samples)
**Models Evaluated:** 7 state-of-the-art vision-language models

---

## Executive Summary

This report presents a comprehensive evaluation of 7 leading multimodal embedding models on the MS-COCO Karpathy benchmark (5,000 test images) with bidirectional retrieval evaluation (Text-to-Image and Image-to-Text). We additionally evaluate compositional reasoning capabilities using the Winoground dataset (400 challenging image-text pairs).

### Key Findings

1. **Apple DFN5B-H achieves state-of-the-art performance** with 50.1% T2I R@1, establishing new SOTA on COCO Karpathy
2. **LAION-CLIP-H offers the best speed-accuracy tradeoff** at 46.3% T2I R@1 and 83.8 QPS
3. **I2T asymmetry is consistent across dense models** with 16-24pp gap (expected due to 5 captions per image)
4. **ColPali exhibits unique symmetric behavior** with only 3.9pp I2T advantage, unlike all other models
5. **Compositional reasoning remains challenging** with Apple DFN5B leading at 35.2% Winoground Image score

### Performance Hierarchy

**Tier 1: High-Performance Dense Models (45-50% T2I R@1)**
- Apple-DFN5B-H: 50.1% (SOTA, but slower at 34.4 QPS)
- LAION-CLIP-H: 46.3% (best balance: high accuracy + 83.8 QPS)
- MetaCLIP-H14: 45.8% (strong open-source alternative)

**Tier 2: Specialized & Mid-Range (39-45% T2I R@1)**
- ColPali-v1.3: 44.9% (late-interaction, very slow at 2.9 QPS)
- Jina-CLIP-v1: 39.3% (balanced performance)

**Tier 3: Efficient Baselines (34-36% T2I R@1)**
- SigLIP-400M: 35.4% (fast at 47.1 QPS, good efficiency)
- OpenAI-CLIP-L: 34.4% (legacy baseline)

---

## 1. Detailed Results

### 1.1 Text-to-Image Retrieval (Primary Metric)

Text-to-Image retrieval uses the first caption of each image as the query, ranking 5,000 images to find the correct match.

| Rank | Model | R@1 | R@5 | R@10 | Architecture |
|:----:|:------|:---:|:---:|:----:|:-------------|
| ðŸ¥‡ | **Apple-DFN5B-H** | **50.1%** | **74.1%** | **82.6%** | Dense (ViT-H/14) |
| ðŸ¥ˆ | **LAION-CLIP-H** | **46.3%** | **70.9%** | **79.7%** | Dense (ViT-H/14) |
| ðŸ¥‰ | **MetaCLIP-H14** | **45.8%** | **70.6%** | **79.5%** | Dense (ViT-H/14) |
| 4 | ColPali-v1.3 | 44.9% | 68.3% | 77.6% | Late-interaction |
| 5 | Jina-CLIP-v1 | 39.3% | 64.6% | 74.2% | Dense (ViT-L/14) |
| 6 | SigLIP-400M | 35.4% | 59.1% | 68.1% | Dense (ViT-So/14) |
| 7 | OpenAI-CLIP-L | 34.4% | 59.1% | 69.1% | Dense (ViT-L/14) |

**Key Observations:**
- Apple DFN5B-H achieves 50.1% R@1, outperforming LAION-CLIP-H by **+3.8pp** despite similar ViT-H/14 architecture
- Top 3 models all use ViT-H/14 backbone, demonstrating importance of model scale
- R@5 scores show 20-24pp improvement over R@1, indicating correct images often appear in top-5

### 1.2 Image-to-Text Retrieval (Bidirectional Evaluation)

Image-to-Text retrieval queries each image against 25,000 captions (5 per image). A retrieval is correct if ANY of the 5 ground-truth captions appears in top-K.

| Rank | Model | R@1 | R@5 | R@10 | I2T Advantage |
|:----:|:------|:---:|:---:|:----:|:-------------:|
| ðŸ¥‡ | **Apple-DFN5B-H** | **69.7%** | **89.2%** | **94.1%** | **+19.6pp** |
| ðŸ¥ˆ | **LAION-CLIP-H** | **66.3%** | **86.5%** | **91.9%** | **+20.0pp** |
| ðŸ¥‰ | **MetaCLIP-H14** | **65.9%** | **86.0%** | **91.9%** | **+20.1pp** |
| 4 | OpenAI-CLIP-L | 58.0% | 80.6% | 87.8% | +23.6pp |
| 5 | Jina-CLIP-v1 | 55.3% | 78.4% | 86.3% | +16.0pp |
| 6 | ColPali-v1.3 | 48.8% | 72.9% | 81.6% | +3.9pp |
| 7 | SigLIP-400M | 45.1% | 64.9% | 72.0% | +9.7pp |

**Key Observations:**
- **Expected asymmetry:** Dense models show 16-24pp I2T advantage due to 5 captions per image
- **ColPali exception:** Only +3.9pp gap (nearly symmetric), unique to late-interaction architecture
- **Ranking shuffle:** OpenAI-CLIP-L performs better at I2T (4th place) than T2I (7th place)

### 1.3 Compositional Reasoning (Winoground)

Winoground evaluates fine-grained compositional understanding with 400 challenging image-text pairs requiring precise attribute binding and spatial reasoning.

| Rank | Model | Image Score | Text Score | Group Score |
|:----:|:------|:-----------:|:----------:|:-----------:|
| ðŸ¥‡ | **Apple-DFN5B-H** | **35.2%** | 14.5% | **12.8%** |
| ðŸ¥ˆ | **Jina-CLIP-v1** | **29.0%** | 7.5% | 5.0% |
| ðŸ¥‰ | **LAION-CLIP-H** | **29.2%** | 12.5% | 10.2% |
| 4 | MetaCLIP-H14 | 28.5% | 13.8% | 9.8% |
| 5 | OpenAI-CLIP-L | 24.5% | 11.5% | 7.2% |
| 6 | ColPali-v1.3 | 24.2% | **15.8%** | 10.2% |
| 7 | SigLIP-400M | 15.8% | 10.8% | 5.0% |

**Metrics Explanation:**
- **Image Score:** Given 2 images and 2 captions, correctly match both images to captions
- **Text Score:** Given 2 images and 2 captions, correctly match both captions to images
- **Group Score:** Both Image and Text scores correct simultaneously (most challenging)

**Key Observations:**
- **Image compositional understanding consistently stronger than text** across all models
- Apple DFN5B achieves highest Image (35.2%) and Group (12.8%) scores
- ColPali achieves highest Text score (15.8%) despite mid-tier Image performance
- Even best models struggle with Group score (12.8%), showing room for improvement

### 1.4 Performance Metrics (Speed & Efficiency)

Performance measured on NVIDIA A40 (48GB) with optimized batch sizes per model.

| Model | QPS (Queries/sec) | Time (5000 imgs) | Batch Size | Speedup vs ColPali |
|:------|:-----------------:|:----------------:|:----------:|:------------------:|
| **LAION-CLIP-H** | **83.8** | 59.6s | 32 | **28.9x** |
| **MetaCLIP-H14** | **76.3** | 65.5s | 32 | **26.3x** |
| OpenAI-CLIP-L | 60.6 | 84.8s | 32 | 20.9x |
| SigLIP-400M | 47.1 | 106.2s | 32 | 16.2x |
| Apple-DFN5B-H | 34.4 | 146.4s | 32 | 11.9x |
| Jina-CLIP-v1 | 25.8 | 194.1s | 32 | 8.9x |
| ColPali-v1.3 | 2.9 | 1733.4s | 4 | 1.0x (baseline) |

**Key Observations:**
- **LAION-CLIP-H offers best speed-accuracy balance:** 46.3% T2I R@1 at 83.8 QPS
- **Apple DFN5B trades speed for accuracy:** 2.4x slower than LAION but +3.8pp higher T2I R@1
- **ColPali is 29x slower than LAION** due to late-interaction scoring (queries 128 text tokens Ã— 1030 image patches)
- **Batch size optimization critical:** ColPali limited to batch=4 due to memory, others use batch=32

---

## 2. Key Findings & Analysis

### 2.1 Apple DFN5B-H Achieves State-of-the-Art

Apple's DFN5B-CLIP-ViT-H-14-378 establishes new SOTA on MS-COCO Karpathy:

| Metric | Apple DFN5B | LAION-CLIP-H | Improvement |
|:-------|:-----------:|:------------:|:-----------:|
| T2I R@1 | 50.1% | 46.3% | +3.8pp |
| T2I R@10 | 82.6% | 79.7% | +2.9pp |
| I2T R@1 | 69.7% | 66.3% | +3.4pp |
| Winoground Image | 35.2% | 29.2% | +6.0pp |

**Analysis:**
- Both models use ViT-H/14 backbone trained on large-scale datasets
- Apple's superior performance likely due to:
  - **Improved training procedure** (DFN = Distilled Foundation Network)
  - **Higher-quality training data curation**
  - **Better caption quality** in pre-training corpus
- **Compositional reasoning advantage is significant:** +6.0pp on Winoground Image score
- **Trade-off:** 2.4x slower than LAION (34.4 vs 83.8 QPS)

**Recommendation:** Use Apple DFN5B when accuracy is paramount; use LAION-CLIP-H for production systems requiring high throughput.

### 2.2 Image-to-Text Asymmetry Analysis

All dense models exhibit I2T > T2I asymmetry, but magnitude varies:

| Model | T2I R@1 | I2T R@1 | Gap | Category |
|:------|:-------:|:-------:|:---:|:---------|
| OpenAI-CLIP-L | 34.4% | 58.0% | +23.6pp | High asymmetry |
| LAION-CLIP-H | 46.3% | 66.3% | +20.0pp | High asymmetry |
| MetaCLIP-H14 | 45.8% | 65.9% | +20.1pp | High asymmetry |
| Apple-DFN5B-H | 50.1% | 69.7% | +19.6pp | High asymmetry |
| Jina-CLIP-v1 | 39.3% | 55.3% | +16.0pp | Moderate asymmetry |
| SigLIP-400M | 35.4% | 45.1% | +9.7pp | Low asymmetry |
| **ColPali-v1.3** | **44.9%** | **48.8%** | **+3.9pp** | **Nearly symmetric** |

**Root Cause:** I2T protocol allows matching ANY of 5 captions per image, creating 5x more "correct" targets:
- T2I: 1 query â†’ find 1 correct image among 5,000 (0.02% density)
- I2T: 1 query â†’ find 5 correct captions among 25,000 (0.02% density per caption, but 5 targets)

**Why ColPali is Different:**
- Dense models: Single embedding comparison â†’ asymmetry from multi-caption advantage
- ColPali: Late-interaction (128 text tokens Ã— 1030 image patches) â†’ fine-grained matching benefits both directions equally

**Implication:** ColPali's symmetric behavior makes it more predictable for production systems, but at 29x slower speed.

### 2.3 Speed-Accuracy Tradeoff

Models fall into distinct efficiency tiers:

| Efficiency Tier | Models | T2I R@1 Range | QPS Range | Use Case |
|:----------------|:-------|:-------------:|:---------:|:---------|
| **High Speed, Moderate Accuracy** | LAION-CLIP-H, MetaCLIP-H14 | 45-47% | 76-84 | Production search engines |
| **Balanced** | OpenAI-CLIP-L, SigLIP-400M, Apple-DFN5B-H | 34-50% | 34-61 | General purpose |
| **Accuracy-Focused** | Jina-CLIP-v1 | 39% | 26 | Specialized applications |
| **Late-Interaction** | ColPali-v1.3 | 45% | 2.9 | Research / small-scale |

**Pareto Frontier:**

```
T2I R@1
50% â”¤         â— Apple (34.4 QPS)
    â”‚
46% â”¤   â— LAION (83.8 QPS) â† Best balance
    â”‚   â— MetaCLIP (76.3 QPS)
    â”‚
45% â”¤ â— ColPali (2.9 QPS)
    â”‚
40% â”¤       â— Jina (25.8 QPS)
    â”‚
35% â”¤               â— SigLIP (47.1 QPS)
    â”‚   â— OpenAI (60.6 QPS)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ QPS
```

**Recommendation by Use Case:**
- **Large-scale production (>1M images):** LAION-CLIP-H (best QPS, 46.3% accuracy)
- **Maximum accuracy:** Apple-DFN5B-H (50.1% T2I R@1, acceptable 34.4 QPS)
- **Resource-constrained:** SigLIP-400M (47.1 QPS, 35.4% accuracy, smaller model)
- **Research / fine-grained matching:** ColPali-v1.3 (44.9% accuracy, unique late-interaction)

### 2.4 Compositional Reasoning Remains Challenging

Winoground results reveal significant room for improvement in fine-grained understanding:

**Best Scores:**
- Image: 35.2% (Apple DFN5B)
- Text: 15.8% (ColPali)
- Group: 12.8% (Apple DFN5B)

**Analysis:**
- **Image > Text across all models:** Models better at discriminating images given text than vice versa
- **Group score dramatically lower:** Only 12.8% vs 35.2% Image score for Apple DFN5B
  - Requires BOTH directions correct simultaneously
  - Exposes weakness in bidirectional compositional understanding
- **Late-interaction advantage in Text score:** ColPali achieves 15.8% (highest) despite mid-tier Image score
  - Fine-grained token-patch alignment helps with text discrimination

**Implications for Real-World Applications:**
- Models may struggle with queries requiring precise attribute binding (e.g., "red car next to blue house")
- Image search more reliable than text search for compositional queries
- Consider query reformulation or late-interaction models for applications requiring fine-grained matching

---

## 3. Technical Methodology

### 3.1 Datasets

#### MS-COCO Karpathy Split
- **Source:** `yerevann/coco-karpathy` (HuggingFace)
- **Split:** Test set (5,000 images)
- **Captions:** 5 human-annotated captions per image (25,000 total)
- **Domain:** General-purpose natural images (people, animals, objects, scenes)
- **Standard Protocol:**
  - T2I: Use 1st caption per image as query (5,000 queries)
  - I2T: Use all 5 captions, mark retrieval correct if ANY in top-K

#### Winoground
- **Source:** `facebook/winoground` (HuggingFace)
- **Size:** 400 challenging image-text pairs
- **Task:** Given 2 images (c0_i0, c1_i1) and 2 captions (c0, c1), correctly match both pairs
- **Difficulty:** Requires fine-grained compositional reasoning (e.g., "dog biting man" vs "man biting dog")
- **Metrics:**
  - Image Score: % where both images correctly matched to captions
  - Text Score: % where both captions correctly matched to images
  - Group Score: % where both Image and Text scores correct

### 3.2 Evaluation Protocol

#### Bidirectional Retrieval
1. **Text-to-Image (T2I):**
   - Query: 1st caption per image (5,000 queries)
   - Gallery: 5,000 images
   - Scoring: Cosine similarity between text and image embeddings
   - Metrics: R@1, R@5, R@10

2. **Image-to-Text (I2T):**
   - Query: Each image (5,000 queries)
   - Gallery: All 25,000 captions (5 per image)
   - Scoring: Cosine similarity (or late-interaction for ColPali)
   - Success: ANY of 5 ground-truth captions in top-K
   - Metrics: R@1, R@5, R@10

#### Winoground Protocol
- For each sample with (c0, c1, c0_i0, c1_i1):
  - Compute scores: s(c0, c0_i0), s(c0, c1_i1), s(c1, c0_i0), s(c1, c1_i1)
  - Image correct: s(c0, c0_i0) > s(c0, c1_i1) AND s(c1, c1_i1) > s(c1, c0_i0)
  - Text correct: s(c0, c0_i0) > s(c1, c0_i0) AND s(c1, c1_i1) > s(c0, c1_i1)
  - Group correct: Image AND Text both correct

### 3.3 Implementation Details

#### Hardware
- **GPU:** NVIDIA A40 (48GB VRAM)
- **Setup:** 2 separate RunPod instances
  - Pod 1 (PyTorch 2.4): ColPali, SigLIP, LAION, Jina, MetaCLIP
  - Pod 2 (PyTorch 2.8): OpenAI, Apple (requires torch 2.6+ for security fix CVE-2025-32434)

#### Software
- **PyTorch:** 2.4.0 (main), 2.8.0 (OpenAI/Apple)
- **Transformers:** Latest (4.x)
- **Precision:** bfloat16 (A40 supports native bf16)
- **Batch Sizes:**
  - ColPali: 4 (memory-constrained due to late-interaction)
  - All others: 32 (optimized for throughput)

#### Model Loading
- **Dense Models (6):** Standard CLIP-style architectures
  - AutoModel.from_pretrained() with torch_dtype=bfloat16
  - Embeddings: L2-normalized, cosine similarity for retrieval
- **ColPali (1):** Late-interaction architecture
  - Outputs: 128 text token embeddings Ã— 1030 image patch embeddings
  - Scoring: MaxSim aggregation over token-patch pairs

#### Multi-Run Setup
- **Runs:** 3 (seeds: 42, 43, 44)
- **Results:** Mean Â± Std reported
- **Note:** Std = 0.0 for all metrics (dataset fixed, no sampling variation)

### 3.4 Known Limitations

1. **Zero Standard Deviation:** Multi-seed runs produce identical results because:
   - Same 5,000 images evaluated each run
   - No random sampling or data augmentation
   - To measure true variance, would need cross-validation or bootstrap sampling

2. **I2T Multi-Caption Protocol:** I2T appears easier due to 5 captions per image
   - Standard protocol in COCO benchmark literature
   - Alternative: Use only 1 caption for symmetric comparison (not standard)

3. **Winoground Dataset Size:** Only 400 samples limits statistical power
   - Larger compositional reasoning benchmarks needed
   - Consider ARO, SugarCrepe, or Crepe for additional evaluation

4. **Batch Size Variation:** ColPali uses batch=4 vs batch=32 for others
   - Due to memory constraints from late-interaction
   - QPS comparison fair (measures end-to-end throughput)

---

## 4. Model Selection Guide

### 4.1 Decision Matrix

| Priority | Recommended Model | T2I R@1 | QPS | Rationale |
|:---------|:------------------|:-------:|:---:|:----------|
| **Maximum Accuracy** | Apple-DFN5B-H | 50.1% | 34.4 | SOTA performance, +3.8pp over LAION |
| **Production Balance** | LAION-CLIP-H | 46.3% | 83.8 | Best speed-accuracy: 2.4x faster than Apple, -3.8pp accuracy |
| **Open-Source** | MetaCLIP-H14 | 45.8% | 76.3 | Competitive with LAION, fully open training data |
| **Late-Interaction** | ColPali-v1.3 | 44.9% | 2.9 | Fine-grained matching, research use |
| **Resource-Constrained** | SigLIP-400M | 35.4% | 47.1 | Smaller model, good efficiency |
| **Balanced Mid-Tier** | Jina-CLIP-v1 | 39.3% | 25.8 | Good balance, multilingual support |

### 4.2 Use Case Recommendations

#### Large-Scale Production Search (>1M images)
**Recommended:** LAION-CLIP-H
- **Why:** 83.8 QPS throughput critical for scale, 46.3% accuracy sufficient
- **Deployment:** Vector database (Pinecone, Weaviate, Qdrant) with HNSW indexing
- **Expected Performance:** Sub-100ms query latency at 1M images

#### High-Accuracy Applications (Medical, Legal, Scientific)
**Recommended:** Apple-DFN5B-H
- **Why:** 50.1% T2I R@1 maximizes precision, 35.2% Winoground Image for compositional understanding
- **Deployment:** Accept 2.4x slower throughput for accuracy gains
- **Expected Performance:** ~30 QPS acceptable for specialized domains

#### Open-Source / Reproducible Research
**Recommended:** MetaCLIP-H14
- **Why:** Fully open training data (CC-2.5B curated), competitive performance (45.8% T2I)
- **Advantage:** Complete transparency in training procedure
- **Alternative:** LAION-CLIP-H if training transparency less critical

#### Fine-Grained Compositional Matching
**Recommended:** ColPali-v1.3
- **Why:** Late-interaction enables fine-grained reasoning, highest Winoground Text score (15.8%)
- **Deployment:** Small-scale (<100K images) or offline batch processing
- **Trade-off:** 2.9 QPS limits real-time applications

#### Mobile / Edge Deployment
**Recommended:** SigLIP-400M
- **Why:** Smaller model size (400M params), 47.1 QPS, acceptable 35.4% T2I
- **Deployment:** On-device search, resource-constrained environments
- **Optimization:** Quantize to int8 for further speedup

---

## 5. Comparison with Prior Benchmarks

### 5.1 Historical Context

#### COCO Karpathy Test (5000 images) - Published Baselines

| Model | Paper | T2I R@1 | I2T R@1 | Year |
|:------|:------|:-------:|:-------:|:----:|
| CLIP (ResNet-50) | OpenAI 2021 | 32.9% | 50.8% | 2021 |
| CLIP (ViT-L/14) | OpenAI 2021 | 36.5% | 54.1% | 2021 |
| **OpenAI-CLIP-L-336 (Ours)** | **This work** | **34.4%** | **58.0%** | **2025** |
| ALIGN | Google 2021 | 42.1% | 59.9% | 2021 |
| **LAION-CLIP-H (Ours)** | **This work** | **46.3%** | **66.3%** | **2025** |
| **Apple-DFN5B-H (Ours)** | **This work** | **50.1%** | **69.7%** | **2025** |

**Notes:**
- Our OpenAI-CLIP-L-336 (34.4%) underperforms published ViT-L/14 (36.5%) due to resolution (336 vs 224)
- Apple DFN5B-H establishes new SOTA at 50.1% T2I R@1

### 5.2 Model Architecture Evolution

| Generation | Representative | Architecture | T2I R@1 | Training Data |
|:-----------|:--------------|:-------------|:-------:|:--------------|
| Gen 1 (2021) | OpenAI CLIP ViT-L | Dense, ViT-L/14 | 36.5% | WIT-400M |
| Gen 2 (2022) | LAION CLIP-H | Dense, ViT-H/14 | 46.3% | LAION-2B |
| Gen 3 (2023) | MetaCLIP-H | Dense, ViT-H/14 + better curation | 45.8% | CC-2.5B |
| Gen 4 (2024) | Apple DFN5B | Dense, ViT-H/14 + distillation | 50.1% | Proprietary |
| Alt: Late-Int | ColPali-v1.3 | Multi-vector, PaliGemma | 44.9% | Mixed (documents) |

**Key Takeaways:**
1. **Scale matters:** ViT-L â†’ ViT-H yields +10pp improvement
2. **Data quality matters:** MetaCLIP shows curation > raw scale
3. **Training technique matters:** Apple's distillation adds +4pp over LAION
4. **Architecture diversity:** Late-interaction offers unique trade-offs

---

## 6. Future Work & Open Questions

### 6.1 Benchmark Improvements

1. **Statistical Variance Measurement:**
   - Current: Multi-seed produces Â± 0.0 (fixed dataset)
   - Proposed: Cross-validation or bootstrap sampling for true variance
   - Benefit: Confidence intervals for model comparisons

2. **Symmetric I2T Protocol:**
   - Current: I2T uses all 5 captions (inflates scores)
   - Proposed: Use 1 caption per image for both directions
   - Benefit: Fair bidirectional comparison

3. **Extended Compositional Evaluation:**
   - Current: Winoground (400 samples)
   - Proposed: Add ARO (1000), SugarCrepe (3000), Crepe (2000)
   - Benefit: More robust compositional reasoning assessment

### 6.2 Model Analysis

1. **Investigate Apple DFN5B Training:**
   - What specific distillation techniques used?
   - What is the teacher model?
   - Can approach be reproduced with open-source models?

2. **ColPali Symmetric Behavior:**
   - Why does late-interaction eliminate I2T asymmetry?
   - Can dense models be modified to achieve similar symmetry?
   - Is symmetry desirable for production systems?

3. **Efficiency Optimization:**
   - Can Apple DFN5B be distilled to smaller models?
   - What is the speed-accuracy Pareto frontier with quantization?
   - Can ColPali's late-interaction be accelerated with sparse attention?

### 6.3 Application Studies

1. **Domain Transfer:**
   - How do models perform on domain-specific data (medical, satellite, art)?
   - Does COCO ranking transfer to specialized domains?

2. **Multilingual Evaluation:**
   - Evaluate on MS-COCO-CN (Chinese), Multi30K (German)
   - Does Jina-CLIP's multilingual training improve non-English performance?

3. **Real-World Deployment:**
   - Latency-throughput analysis under production constraints
   - Vector database indexing overhead (HNSW, IVF)
   - End-to-end system performance (embedding + retrieval + re-ranking)

---

## 7. Conclusion

This comprehensive evaluation of 7 state-of-the-art multimodal embedding models on MS-COCO Karpathy reveals:

1. **Apple DFN5B-H sets new SOTA** at 50.1% T2I R@1, demonstrating continued progress in vision-language pre-training
2. **LAION-CLIP-H offers production-ready balance** with 46.3% accuracy at 83.8 QPS (2.4x faster than Apple)
3. **Compositional reasoning remains challenging** with best Image score of 35.2% on Winoground
4. **Late-interaction architectures exhibit unique properties** (ColPali's symmetric I2T behavior) worth further investigation

### Key Recommendations

- **Production systems:** LAION-CLIP-H for optimal speed-accuracy tradeoff
- **Maximum accuracy:** Apple-DFN5B-H for SOTA performance
- **Open-source:** MetaCLIP-H14 for reproducible research
- **Fine-grained matching:** ColPali-v1.3 for specialized applications

### Impact

This benchmark provides the community with:
- **Comprehensive evaluation** of leading models on standardized protocol
- **Reproducible methodology** for future model comparisons
- **Actionable guidance** for practitioners selecting models for deployment

---

## 8. Reproducibility

### 8.1 Code Availability

All evaluation code available at: [Your Repository URL]

Key files:
- `run_benchmark_grand_slam_v28_publication_ready.py` - Main benchmark (5 models)
- `run_benchmark_v28_openai_apple.py` - PyTorch 2.8+ benchmark (2 models)
- `benchmark_v28_all_models_combined.csv` - Combined results

### 8.2 Hardware Requirements

- **Minimum:** NVIDIA A40 (48GB) or A100 (40GB)
- **Recommended:** NVIDIA A100 (80GB) for larger batch sizes
- **Runtime:** ~2-3 hours for all 7 models (single run)

### 8.3 Software Requirements

**Pod 1 (Main - 5 models):**
```bash
pip install torch==2.4.0 transformers datasets pillow timm einops pandas tabulate
```

**Pod 2 (OpenAI/Apple - 2 models):**
```bash
# Requires PyTorch 2.8+ for CVE-2025-32434 fix
pip install torch==2.8.0 transformers datasets pillow pandas tabulate
```

### 8.4 Running the Benchmark

**Single run:**
```bash
python run_benchmark_grand_slam_v28_publication_ready.py --runs 1 --batch-size 32
```

**Multi-run (3 seeds):**
```bash
python run_benchmark_grand_slam_v28_publication_ready.py --runs 3 --batch-size 32
```

**OpenAI + Apple (separate pod):**
```bash
python run_benchmark_v28_openai_apple.py --runs 3 --batch-size 32
```

