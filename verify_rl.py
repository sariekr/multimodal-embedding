import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# 1. AYARLAR
base_model_id = "OpenPipe/Qwen3-14B-Instruct"
lora_path = "qwen-rl-pro-result" # Senin eÄŸitim klasÃ¶rÃ¼n

# 2. DATASET
with open("dataset.json", "r") as f:
    dataset = json.load(f)

print("â³ Modeller YÃ¼kleniyor (Bu biraz sÃ¼rebilir)...")

# A. Ana Modeli YÃ¼kle
tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# B. LoRA AdaptÃ¶rÃ¼nÃ¼ Ãœzerine Giy ve BÄ°RLEÅTÄ°R (Merge)
print(f"ğŸ› ï¸ LoRA AdaptÃ¶rÃ¼ YÃ¼kleniyor: {lora_path}")
model = PeftModel.from_pretrained(base_model, lora_path)
model = model.merge_and_unload() # <--- Ä°ÅTE SÄ°HÄ°RLÄ° KOMUT (Tek parÃ§a haline getirir)
print("âœ… Model BaÅŸarÄ±yla BirleÅŸtirildi!")

# 3. TEST FONKSÄ°YONU
def generate_answer(prompt):
    messages = [
        {"role": "system", "content": "You are a strict data extraction engine.\nRULES:\n1. Output ONLY a JSON object.\n2. DO NOT use <think> tags.\n3. Allowed categories: [\"BILLING\", \"TECHNICAL\", \"SHIPPING\", \"PRODUCT\", \"OTHER\"]."},
        {"role": "user", "content": prompt}
    ]
    
    # Qwen'in kendi chat ÅŸablonunu kullan (Manuel string formatlama hatasÄ±nÄ± Ã¶nler)
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.1, # YaratÄ±cÄ±lÄ±ÄŸÄ± kÄ±sÄ±p netlik istiyoruz
            do_sample=False  # Greedy decoding (En olasÄ± cevabÄ± seÃ§)
        )
        
    # Sadece yeni Ã¼retilen kÄ±smÄ± al
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

# 4. BENCHMARK BAÅLASIN
stats = {"total": 0, "correct_format": 0, "no_think": 0}

print("\n" + "="*80)
print("ğŸš€ GARANTÄ°LÄ° DOÄRULAMA TESTÄ°")
print("="*80)

for i, item in enumerate(dataset):
    prompt = item['prompt']
    response = generate_answer(prompt)
    
    # Analiz
    has_think = "<think>" in response
    is_json = response.strip().startswith("{")
    
    stats["total"] += 1
    if not has_think: stats["no_think"] += 1
    if is_json and not has_think: stats["correct_format"] += 1
    
    # Ä°lk 5 Ã¶rneÄŸi gÃ¶ster
    if i < 5:
        print(f"SORU: {prompt[:40]}...")
        print(f"CEVAP: {response}")
        print("-" * 40)

# 5. SONUÃ‡
print("\n" + "="*60)
print(f"Toplam Veri: {stats['total']}")
print(f"âœ… Sessizlik (No Think): %{stats['no_think']/stats['total']*100:.1f}")
print(f"âœ… JSON Format BaÅŸarÄ±sÄ±: %{stats['correct_format']/stats['total']*100:.1f}")
print("="*60)