import os
import torch
import json
import re
from datasets import load_from_disk, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig
from trl import GRPOConfig, GRPOTrainer

# 1. AYARLAR
model_id = "OpenPipe/Qwen3-14B-Instruct"
output_dir = "qwen-rl-hard-bureaucrat"

# 2. ZORLU Ã–DÃœL FONKSÄ°YONU (BÃ¼rokrat MantÄ±ÄŸÄ±)
def reward_function(completions, prompts, **kwargs):
    rewards = []
    
    for prompt, completion in zip(prompts, completions):
        # CevabÄ± al
        try:
            if isinstance(completion, list):
                response_text = completion[0]['content']
            elif hasattr(completion, 'content'):
                response_text = completion.content
            else:
                response_text = str(completion)
            
            # Prompt'u string'e Ã§evir
            prompt_text = str(prompt)
            # System prompt kÄ±smÄ±nÄ± at, sadece user mesajÄ±na bak (Daha temiz analiz iÃ§in)
            if "user\n" in prompt_text:
                user_content = prompt_text.split("user\n")[1].split("<|im_end|>")[0].lower()
            else:
                user_content = prompt_text.lower()
                
        except:
            rewards.append(0.0)
            continue

        score = 0.0

        # --- A. FORMAT CEZALARI ---
        if "<think>" in response_text or "</think>" in response_text:
            score -= 20.0 # DÃ¼ÅŸÃ¼nmek yasak!
        
        clean_text = response_text.strip()
        if not clean_text.startswith("{"):
            score -= 5.0
        else:
            score += 2.0 # JSON formatÄ±na teÅŸvik

        # --- B. MANTIK MOTORU (GROUND TRUTH HESAPLAMA) ---
        # 1. FiyatÄ± Bul ($ simgesinden sonraki sayÄ±)
        price = 0
        price_match = re.search(r'\$(\d+)', user_content)
        if price_match:
            price = int(price_match.group(1))
        
        # 2. Tonu Bul (Kibar mÄ±?)
        is_polite = any(w in user_content for w in ["please", "kindly", "appreciate", "help", "thank"])
        
        # 3. KURAL SETÄ° (HIYERARÅžÄ°)
        target_category = "UNKNOWN"
        
        if price < 10:
            target_category = "IGNORE"
        elif price > 2000:
            target_category = "VIP_DESK"
        elif is_polite:
            target_category = "AUTO_BOT"
        else:
            target_category = "HUMAN_AGENT" # VarsayÄ±lan: Sinirli/Kaba insan

        # --- C. KARÅžILAÅžTIRMA ---
        try:
            data = json.loads(clean_text)
            model_category = data.get("category", "UNKNOWN")
            
            if model_category == target_category:
                score += 20.0 # TAM Ä°SABET!
            else:
                score -= 10.0 # YANLIÅž KATEGORÄ° CEZASI
                
                # Modelin nerede hata yaptÄ±ÄŸÄ±nÄ± anlamak iÃ§in (Opsiyonel ceza)
                # EÄŸer IGNORE olmasÄ± gerekirken HUMAN dediyse daha Ã§ok kÄ±zabiliriz
                if target_category == "IGNORE" and model_category != "IGNORE":
                    score -= 5.0 # Fakirleri sakÄ±n insanla gÃ¶rÃ¼ÅŸtÃ¼rme!
                    
        except:
            score -= 5.0 # JSON parse edilemedi

        rewards.append(score)
    return rewards

# 3. MODELÄ° YÃœKLE
print(f"Model yÃ¼kleniyor: {model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=torch.bfloat16, 
    device_map="auto",
    trust_remote_code=True
)

# 4. DATASET HAZIRLIÄžI (dataset_hard.json kullanÄ±yoruz)
if not os.path.exists("dataset_hard.json"):
    raise FileNotFoundError("Ã–nce dataset generator kodunu Ã§alÄ±ÅŸtÄ±rÄ±p dataset_hard.json Ã¼retmelisin!")

with open("dataset_hard.json", "r") as f:
    raw_data = json.load(f)

# System Prompt artÄ±k yeni kurallarÄ± iÃ§eriyor
system_prompt = """You are a strict automated routing system.
RULES:
1. Output ONLY a JSON object: {"category": "..."}
2. DO NOT use <think> tags.
3. Allowed categories: ["IGNORE", "VIP_DESK", "HUMAN_AGENT", "AUTO_BOT"].
4. LOGIC HIERARCHY:
   - Value < $10 -> IGNORE
   - Value > $2000 -> VIP_DESK
   - Value $10-$2000 AND Polite -> AUTO_BOT
   - Value $10-$2000 AND Angry -> HUMAN_AGENT"""

formatted_data = []
for item in raw_data:
    formatted_data.append({
        "prompt": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": item['prompt']}
        ]
    })

dataset = Dataset.from_list(formatted_data)
print(f"Dataset yÃ¼klendi: {len(dataset)} Ã¶rnek.")

# 5. LORA KONFIG
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    task_type="CAUSAL_LM",
    lora_dropout=0.05,
    bias="none"
)

# 6. EÄžÄ°TÄ°M AYARLARI (Zor gÃ¶rev olduÄŸu iÃ§in 3 Epoch ÅŸart)
training_args = GRPOConfig(
    output_dir=output_dir,
    learning_rate=1e-5,            # YavaÅŸ ve emin adÄ±mlarla Ã¶ÄŸrensin
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8, 
    
    num_generations=4,             # A100 iÃ§in gÃ¼venli sayÄ±
    num_train_epochs=3,            # 3 tur dÃ¶nsÃ¼n, kurallar otursun
    
    max_prompt_length=512,
    max_completion_length=200,
    gradient_checkpointing=True,
    logging_steps=1,
    save_strategy="no",
    report_to="none"
)

# 7. BAÅžLAT
trainer = GRPOTrainer(
    model=model,
    reward_funcs=reward_function,
    args=training_args,
    train_dataset=dataset,
    peft_config=peft_config,
    processing_class=tokenizer,
)

print("ðŸš€ BÃœROKRAT EÄžÄ°TÄ°MÄ° BAÅžLIYOR (Hard Mode)...")
trainer.train()
trainer.save_model(output_dir)
print(f"âœ… Bitti! Yeni BÃ¼rokrat Modelin ÅŸurada: {output_dir}")