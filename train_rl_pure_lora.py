import os
import torch
import json
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig
from trl import GRPOConfig, GRPOTrainer

# --- 1. AYARLAR ---
model_id = "OpenPipe/Qwen3-14B-Instruct"
output_dir = "qwen-rl-pure-lora-result"

# --- 2. Ã–DÃœL FONKSÄ°YONU (W&B ile Birebir AynÄ±) ---
def reward_function(completions, prompts, **kwargs):
    rewards = []
    
    billing_keywords = ["bill", "charge", "refund", "money", "price", "cost", "pay", "card"]
    technical_keywords = ["bug", "crash", "error", "login", "screen", "app", "broken", "slow"]
    shipping_keywords = ["package", "delivery", "track", "arrive", "ship", "lost", "where"]

    for prompt, completion in zip(prompts, completions):
        # CevabÄ± gÃ¼venli ÅŸekilde al
        try:
            response_text = completion[0]['content'] if isinstance(completion, list) else completion
            prompt_text = str(prompt).lower()
        except:
            rewards.append(0.0)
            continue

        score = 0.0

        # A. SUSMA CEZASI (-20 Puan)
        if "<think>" in response_text or "</think>" in response_text:
            score -= 20.0
        
        # B. FORMAT
        clean_text = response_text.strip()
        if not clean_text.startswith("{"):
            score -= 5.0
        else:
            score += 2.0
        
        if "```" in clean_text: score -= 5.0

        # C. ZEKA VE MANTIK
        try:
            data = json.loads(clean_text)
            category = data.get("category", "UNKNOWN")
            
            hit = False
            # Keyword EÅŸleÅŸmeleri
            if any(k in prompt_text for k in billing_keywords):
                if category == "BILLING": score += 15.0; hit = True
                elif category == "OTHER": score -= 10.0
            
            elif any(k in prompt_text for k in technical_keywords):
                if category == "TECHNICAL": score += 15.0; hit = True
                elif category == "OTHER": score -= 10.0
            
            elif any(k in prompt_text for k in shipping_keywords):
                if category == "SHIPPING": score += 15.0; hit = True
                elif category == "OTHER": score -= 10.0
            
            # Keyword yoksa OTHER doÄŸru cevaptÄ±r
            if not hit and category == "OTHER": score += 15.0

        except:
            score -= 5.0 # JSON bozuk

        rewards.append(score)
    return rewards

# --- 3. MODELÄ° YÃœKLE (STANDART BFLOAT16 - NO QUANTIZATION) ---
print(f"Model yÃ¼kleniyor (bfloat16): {model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,  # <--- Ä°ÅžTE FARK BURADA (4-bit deÄŸil, tam hassasiyet)
    device_map="auto",
    trust_remote_code=True,
    attn_implementation="flash_attention_2" # Bellek tasarrufu iÃ§in kritik
)

# --- 4. DATASET ---
# Ã–nceki adÄ±mda hazÄ±rladÄ±ÄŸÄ±mÄ±z dataseti yÃ¼kle
if not os.path.exists("rl_dataset"):
    raise ValueError("Dataset bulunamadÄ±! Ã–nce prepare_data.py Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±.")
dataset = load_from_disk("rl_dataset")

# --- 5. LORA AYARLARI ---
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    task_type="CAUSAL_LM",
    lora_dropout=0.05,
    bias="none"
)

# --- 6. TRAINING CONFIG ---
# Bellek yÃ¶netimi iÃ§in batch size 1 ve gradient accumulation yÃ¼ksek tutuldu
training_args = GRPOConfig(
    output_dir=output_dir,
    learning_rate=1e-5,
    per_device_train_batch_size=1,     # VRAM patlamamasÄ± iÃ§in en dÃ¼ÅŸÃ¼kte
    gradient_accumulation_steps=8,     # Sanal batch size'Ä± artÄ±rÄ±yoruz (Stabilite iÃ§in)
    num_generations=4,                 # Her soruda 4 cevap dene (8 yaparsan VRAM yetmeyebilir)
    max_prompt_length=512,
    max_completion_length=300,
    num_train_epochs=1,                # Benchmark iÃ§in 1 epoch yeterli
    logging_steps=1,
    save_steps=50,
    report_to="none"
)

# --- 7. TRAINER BAÅžLAT ---
trainer = GRPOTrainer(
    model=model,
    reward_funcs=reward_function,
    args=training_args,
    train_dataset=dataset,
    peft_config=peft_config,
    tokenizer=tokenizer,
)

print("ðŸš€ RUNPOD 'PURE LORA' RL EÄžÄ°TÄ°MÄ° BAÅžLIYOR...")
print("Not: Bu iÅŸlem yÃ¼ksek VRAM tÃ¼ketir.")
trainer.train()

# --- 8. KAYDET ---
print("EÄŸitim bitti. Adapter kaydediliyor...")
trainer.save_model(output_dir)
print(f"âœ… Model ÅŸuraya kaydedildi: {output_dir}")